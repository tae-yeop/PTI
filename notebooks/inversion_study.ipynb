{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiteam/tykim/generative/gan/PTI\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from configs import paths_config, hyperparameters, global_config\n",
    "from utils.align_data import pre_process_images\n",
    "from scripts.run_pti import run_PTI\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.latent_editor_wrapper import LatentEditorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_multi_id_training = False\n",
    "global_config.device = 'cuda'\n",
    "paths_config.e4e = '/home/aiteam/tykim/generative/gan/PTI/pretrained_models/e4e_ffhq_encode.pt'\n",
    "paths_config.input_data_id = 'iu'\n",
    "paths_config.input_data_path = f'/home/aiteam/tykim/generative/gan/PTI/images/aligned'\n",
    "paths_config.stylegan2_ada_ffhq = '/home/aiteam/tykim/generative/gan/PTI/pretrained_models/ffhq.pkl'\n",
    "paths_config.checkpoints_dir = '/home/aiteam/tykim/generative/gan/PTI/checkpoints'\n",
    "paths_config.style_clip_pretrained_mappers = '/home/aiteam/tykim/generative/gan/PTI/pretrained_models'\n",
    "hyperparameters.use_locality_regularization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from string import ascii_uppercase\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import os\n",
    "from configs import global_config, paths_config\n",
    "import wandb\n",
    "\n",
    "from training.coaches.multi_id_coach import MultiIDCoach\n",
    "from training.coaches.single_id_coach import SingleIDCoach\n",
    "from utils.ImagesDataset import ImagesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_config.input_data_path = '/home/aiteam/tykim/generative/gan/PTI/images/iu-1'\n",
    "\n",
    "dataset = ImagesDataset(paths_config.input_data_path, transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/aiteam/miniconda3/envs/eg3d/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "coach = SingleIDCoach(dataloader, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_bufs = {name: buf for (name, buf) in coach.G.synthesis.named_buffers() if 'noise_const' in name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b4.resample_filter\n",
      "b4.conv1.resample_filter\n",
      "b4.conv1.noise_const\n",
      "b8.resample_filter\n",
      "b8.conv0.resample_filter\n",
      "b8.conv0.noise_const\n",
      "b8.conv1.resample_filter\n",
      "b8.conv1.noise_const\n",
      "b16.resample_filter\n",
      "b16.conv0.resample_filter\n",
      "b16.conv0.noise_const\n",
      "b16.conv1.resample_filter\n",
      "b16.conv1.noise_const\n",
      "b32.resample_filter\n",
      "b32.conv0.resample_filter\n",
      "b32.conv0.noise_const\n",
      "b32.conv1.resample_filter\n",
      "b32.conv1.noise_const\n",
      "b64.resample_filter\n",
      "b64.conv0.resample_filter\n",
      "b64.conv0.noise_const\n",
      "b64.conv1.resample_filter\n",
      "b64.conv1.noise_const\n",
      "b128.resample_filter\n",
      "b128.conv0.resample_filter\n",
      "b128.conv0.noise_const\n",
      "b128.conv1.resample_filter\n",
      "b128.conv1.noise_const\n",
      "b256.resample_filter\n",
      "b256.conv0.resample_filter\n",
      "b256.conv0.noise_const\n",
      "b256.conv1.resample_filter\n",
      "b256.conv1.noise_const\n",
      "b512.resample_filter\n",
      "b512.conv0.resample_filter\n",
      "b512.conv0.noise_const\n",
      "b512.conv1.resample_filter\n",
      "b512.conv1.noise_const\n",
      "b1024.resample_filter\n",
      "b1024.conv0.resample_filter\n",
      "b1024.conv0.noise_const\n",
      "b1024.conv1.resample_filter\n",
      "b1024.conv1.noise_const\n"
     ]
    }
   ],
   "source": [
    "for (name, buf) in coach.G.synthesis.named_buffers():\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg_samples = 10000\n",
    "z_samples = np.random.RandomState(123).randn(w_avg_samples, coach.G.z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
     ]
    }
   ],
   "source": [
    "w_samples = coach.G.mapping(torch.from_numpy(z_samples).to(device), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(1,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0459, -0.1708, -0.0535,  ...,  0.4128, -0.2988,  0.4720],\n",
       "          [-0.1542,  0.6592, -0.5242,  ..., -0.7527,  0.3452, -0.0674],\n",
       "          [ 0.1562, -0.0924,  0.0730,  ..., -0.3694,  0.4368,  0.8599],\n",
       "          ...,\n",
       "          [ 0.7664,  0.5365,  0.3606,  ..., -0.3208, -0.2175,  1.9953],\n",
       "          [ 0.3416, -2.0299, -1.9410,  ...,  1.6554, -0.1197,  0.4033],\n",
       "          [-0.3221, -0.3524, -0.0472,  ..., -0.0982,  0.3853, -0.5688]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(t, shifts=1, dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0078,  0.0091,  0.0122,  ..., -0.1234, -0.1410, -0.0217],\n",
       "          [-0.1016, -0.3456, -0.3329,  ..., -0.2598, -0.0233,  0.0104],\n",
       "          [-0.0144, -0.0067,  0.0052,  ..., -0.1613,  0.3756,  0.1343],\n",
       "          ...,\n",
       "          [ 0.4112,  0.1935,  0.9726,  ...,  0.0698, -0.4340,  1.5291],\n",
       "          [-0.6934,  3.9400, -0.6805,  ..., -0.1981, -0.0483,  0.1378],\n",
       "          [ 0.1135,  0.0166, -0.1083,  ..., -0.0378, -0.2191,  0.1832]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t * torch.roll(t, shifts=1, dims=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for buf in noise_bufs.values():\n",
    "  buf[:] = torch.randn_like(buf)\n",
    "  buf.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reg_loss = 0.0\n",
    "for v in noise_bufs.values():\n",
    "  noise = v[None, None, :, :]\n",
    "  print(noise.requires_grad)\n",
    "  \n",
    "  break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coach.G.mapping.num_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,1,8).repeat([1, 18, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = torch.nn.Parameter(\n",
    "        torch.tensor([0, 0, 0], dtype=torch.float32, device=device))\n",
    "\n",
    "start_z = 3.2\n",
    "\n",
    "position = torch.nn.Parameter(\n",
    "        torch.tensor([0, 0, start_z], dtype=torch.float32, device=device))\n",
    "\n",
    "focal_length =   4#@param {type: \"number\"}\n",
    "focal_length = torch.nn.Parameter(torch.tensor([focal_length],\n",
    "                                               dtype=torch.float32,\n",
    "                                               device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix(angle, dim=0):\n",
    "    s = angle.sin()\n",
    "    c = angle.cos()\n",
    "    dims = [d for d in range(3) if d != dim]\n",
    "    print(dims)\n",
    "    return torch.zeros(9).to(angle).index_add(0, torch.LongTensor([dims[0] + dims[0] * 3,\n",
    "                          dims[1] + dims[0] * 3,\n",
    "                          dims[0] + dims[1] * 3,\n",
    "                          dims[1] + dims[1] * 3, dim + dim * 3\n",
    "                          ]).to(angle.device), torch.cat([c, s, -s, c,\n",
    "                                                          angle * 0 + 1\n",
    "                                                          ])).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(euler):\n",
    "    return (\n",
    "        rotation_matrix(euler[0:1], 0)\n",
    "        @ rotation_matrix(euler[1:2], 1)\n",
    "        @ rotation_matrix(euler[2:3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotate(rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix(rotation[0:1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix(rotation[0:1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix(rotation[0:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_cam_matrix():\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            [0, 0, 0.5],\n",
    "            [0, 0, 0.5],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=torch.float32, device=device).flatten().index_add(\n",
    "            0,\n",
    "            torch.LongTensor([0, 4]).to(device),\n",
    "            torch.cat((focal_length,) * 2)).reshape(3, 3)\n",
    "        \n",
    "\n",
    "def make_c():\n",
    "    matrix = torch.tensor([\n",
    "                        [-1, 0,  0],\n",
    "                        [0, -1,  0],\n",
    "                        [0,  0, -1]\n",
    "    ]).float().to(device)\n",
    "    matrix = matrix @ rotate(rotation)\n",
    "    # Position is not rotated\n",
    "    pos = torch.maximum(torch.tensor([-10, -10, 2.6]).to(position),\n",
    "                        position)\n",
    "    matrix = torch.cat((matrix, pos.unsqueeze(-1)), dim=1)\n",
    "    matrix = torch.cat((matrix, torch.eye(4)[-1:].to(device)), dim=0)\n",
    "    cam_matrix = get_cam_matrix().flatten()\n",
    "    return torch.cat((matrix.flatten(), cam_matrix)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter(\n",
    "        torch.tensor([0, 0, start_z], dtype=torch.float32, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[0, 2]\n",
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000, -1.0000,  3.2000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "         4.0000,  0.0000,  0.5000,  0.0000,  4.0000,  0.5000,  0.0000,  0.0000,\n",
       "         1.0000], device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coach.data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2d5ac534c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "img = torch.randn(3, 12, 12)\n",
    "# GPU 메모리에선 plt를 실행 못함 -> CPU로 불러옴\n",
    "# 배치 차원을 날림\n",
    "# 파이토치는 channel-first이므로 channel-last로 바꿈\n",
    "plt.imshow(img.cpu().squeeze(dim=0).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in coach.data_loader:\n",
    "  print(i[0], i[1])\n",
    "  print(i[1].shape)\n",
    "  plt.imshow(i[1].squeeze().cpu().squeeze(dim=0).permute(1, 2, 0))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('eg3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0757d6925bc23c2f30026c6f10c42ba2226ac99bb9ce241ac526661745f2cf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
